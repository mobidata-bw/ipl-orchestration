# compose "namespace", prepended to all network names, container names, etc.
name: ipl

# Define placeholder for running a container with the same UID/GID as your local user
x-local-user: &local-user ${DOCKER_LOCAL_USER:?Variable needs to be set in .env (e.g. "DOCKER_LOCAL_USER=1000:1000")}

x-ocpdb-defaults: &ocpdb-defaults
  image: ${OCPDB_IMAGE}
  networks: [ipl]
  user: *local-user
  environment:
    OCPDB_POSTGRES_DB: ${OCPDB_POSTGRES_DB:?'missing/empty'}
    OCPDB_POSTGRES_USER: ${OCPDB_POSTGRES_USER:?'missing/empty'}
    OCPDB_POSTGRES_PASSWORD: ${OCPDB_POSTGRES_PASSWORD:?'missing/empty'}
    OCPDB_POSTGRES_HOST: ${OCPDB_POSTGRES_HOST}
    OCPDB_PROJECT_URL: ${OCPDB_PROJECT_URL}
    OCPDB_CELERY_BROKER_URL: ${OCPDB_CELERY_BROKER_URL}
  depends_on:
    ocpdb-db:
      condition: service_healthy
    ocpdb-rabbitmq:
      condition: service_healthy
  volumes:
    - ./etc/ocpdb/config.yaml:/app/config.yaml
    - ./etc/ocpdb/config.secrets.yaml:/app/config.secrets.yaml
    - ./var/ocpdb/logs:/app/logs
    - ./var/ocpdb/temp:/app/temp

x-park-api-defaults: &park-api-defaults
  image: ${PARK_API_IMAGE}
  networks: [ipl]
  user: ${DOCKER_LOCAL_USER}
  environment:
    PARK_API_POSTGRES_DB: ${PARK_API_POSTGRES_DB:?'missing/empty'}
    PARK_API_POSTGRES_USER: ${PARK_API_POSTGRES_USER:?'missing/empty'}
    PARK_API_POSTGRES_PASSWORD: ${PARK_API_POSTGRES_PASSWORD:?'missing/empty'}
    PARK_API_POSTGRES_HOST: ${PARK_API_POSTGRES_HOST}
    PARK_API_PROJECT_URL: ${PARK_API_PROJECT_URL:?'missing/empty'}
    PARK_API_CELERY_BROKER_URL: ${PARK_API_CELERY_BROKER_URL}
  depends_on:
    park-api-db:
      condition: service_healthy
    park-api-rabbitmq:
      condition: service_healthy
  volumes:
    - ./etc/park-api/config.yaml:/app/config.yaml
    - ./etc/park-api/config.secrets.yaml:/app/config.secrets.yaml
    - ./var/park-api/logs:/app/logs
    - ./var/park-api/temp:/app/temp

x-generic-rabbitmq: &generic-rabbitmq
  networks: [ipl]
  image: rabbitmq:3.12
  user: rabbitmq  # required due eacces-issue: https://github.com/docker-library/rabbitmq/issues/318
  environment:
    # Disable spammy logging
    RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS: '-rabbit log [{console,[{level,warning}]}]'
  healthcheck:
    test: rabbitmq-diagnostics -q ping
    interval: 3s
    timeout: 1s
    retries: 20
  restart: unless-stopped

services:
  # Traefik is our ingress: requests enter our infrastructure through it.
  # It is an HTTP reverse proxy, discovering available services (and their containers)
  # automatically by using the Docker API.
  # Trafik can't run as $DOCKER_LOCAL_USER (see &local-user), because service discovery just works with root.
  ingress:
    networks: [ipl]
    image: traefik:v2.10
    ports:
      # todo: change to 80:80
      - "8080:80"
      # # todo: change to 443:443
      # - "8443:443"
      # todo: change to 8080:8080
      # web UI, Prometheus, etc.
      - "8081:8080"
    environment:
      TZ: ${TZ:?'missing/empty $TZ'}
    volumes:
      - ./etc/traefik:/etc/traefik
      - ./var/log/traefik:/var/log/traefik
      # So that Traefik can listen to the Docker events.
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: "traefik healthcheck"
      interval: 3s
      timeout: 1s
      retries: 10
    restart: unless-stopped

  well-known-uris:
    networks: [ipl]
    image: ${CADDY_IMAGE}
    restart: unless-stopped
    ports:
      - 6998:80
    volumes:
      - ./etc/well-known/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./etc/well-known/www:/var/www:ro
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.well-known-uris.rule=PathPrefix(`/.well-known/`) || Path(`/robots.txt`)"
      - "traefik.http.routers.well-known-uris.middlewares=well-known-uris-middleware"
      - "traefik.http.middlewares.well-known-uris-middleware.headers.accesscontrolallowmethods=GET,OPTIONS,HEAD"
      - "traefik.http.middlewares.well-known-uris-middleware.headers.accesscontrolallowheaders=*"
      - "traefik.http.services.well-known-uris.loadbalancer.server.port=80"

  goaccess:
    networks: [ipl]
    user: *local-user
    image: ${GOACCESS_IMAGE}
    environment:
      - TZ=${TZ:?'missing/empty $TZ'}
    volumes:
      # entrypoint script
      - ./bin/goaccess/import.sh:/opt/import.sh:ro
      # GoAccess configuration file
      - ./etc/goaccess.conf:/etc/goaccess.conf:ro
      # data written by GoAccess itself: statistics database, generated HTML report, processing logs
      - ./var/goaccess/data:/srv/goaccess/data
      - ./var/goaccess/report:/srv/goaccess/report
      - ./var/log/goaccess:/srv/goaccess/logs
      # Traefik access logs
      - ./var/log/traefik:/var/log/traefik:ro
    depends_on:
      ingress:
        condition: service_healthy
    entrypoint: ["/opt/import.sh"]
    restart: unless-stopped

  # This service applies a couple of conversions for certain GBFS feeds to fix
  # syntax issues which prevent importing in Lamassu.
  # It needs to be configured as proxy for Lamassu and feeds to be filtered should
  # explicitly be requests using http. Proxy will take care of https requesting and
  # necessary response rewrites.
  transformer-proxy:
    networks: [ipl]
    image: ${TRANSFORMER_PROXY_IMAGE}
    volumes:
      # Mount config file which declares hosts to proxy
      - ./etc/transformer-proxy/config.yaml:/app/config.yaml
    ports:
      # We export the port for debugging purposes. Lamassu acceses via docker network
      - ${TRANSFORMER_PROXY_PORT}:8080
    restart: unless-stopped

  caddy:
    networks: [ipl]
    image: ${CADDY_IMAGE}
    restart: unless-stopped
    ports:
      # public IPL assets
      - 6999:80
      # GoAccess access statistics reports
      - "${GOACCESS_PORT:?'missing/empty env var $GOACCESS_PORT'}:81"
    volumes:
      # Caddy configuration file
      - ./etc/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      # data stored by Caddy itself: certificates, parsed & normalized config, etc.
      - ./var/caddy/data:/data
      - ./var/caddy/config:/config
      # public IPL assets (served on :80)
      - ./var/www/:/var/www:ro
      # GoAccess access statistics reports (served on :81)
      - ./var/goaccess/report:/var/goaccess/report:ro
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.caddy.rule=PathPrefix(`/datasets`)"
      - "traefik.http.routers.caddy.middlewares=caddy-middleware-1,caddy-middleware-2"
      - "traefik.http.middlewares.caddy-middleware-1.stripprefix.prefixes=/datasets"
      - "traefik.http.middlewares.caddy-middleware-2.headers.accesscontrolallowmethods=GET,OPTIONS,HEAD"
      - "traefik.http.middlewares.caddy-middleware-2.headers.accesscontrolallowheaders=*"
      - "traefik.http.services.caddy.loadbalancer.server.port=80"

  # This service requests non-standard sharing provider APIs and generates
  # a gbfs feed intended for local import into Lamassu. The volume needs to be
  # mounted by the lamassu container also.
  x2gbfs:
    networks: [ipl]
    image: ${X2GBFS_IMAGE}
    volumes:
      - ./var/gbfs/feeds:/app/out/
    command: -p ${X2GBFS_PROVIDERS:?missing/empty env var $$X2GBFS_PROVIDERS} -b file:///var/gbfs/feeds -i ${X2GBFS_UPDATE_INTERVAL_SECONDS:?missing/empty env var $$X2GBFS_UPDATE_INTERVAL_SECONDS}
    environment:
      - DEER_API_URL
      - DEER_USER=${DEER_USER:?missing/empty env var $$DEER_USER}
      - DEER_PASSWORD=${DEER_PASSWORD:?'missing/empty env var $$DEER_PASSWORD}
      - VOI_API_URL
      - VOI_USER=${VOI_USER:?missing/empty env var $$VOI_USER}
      - VOI_PASSWORD=${VOI_PASSWORD:?'missing/empty env var $$VOI_PASSWORD}
    restart: unless-stopped

  lamassu:
    networks: [ipl]
    image: ${LAMASSU_IMAGE}
    ports:
      - ${LAMASSU_PORT}:8080
      - ${LAMASSU_ADMIN_PORT}:9001
    environment:
      LAMASSU_DB_CONNECT_CLIENT_ID: ${LAMASSU_DB_CONNECT_CLIENT_ID:?'missing/empty env var $LAMASSU_DB_CONNECT_CLIENT_ID'}
      LAMASSU_DB_CONNECT_API_KEY: ${LAMASSU_DB_CONNECT_API_KEY:?'missing/empty env var $LAMASSU_DB_CONNECT_API_KEY'}
      LAMASSU_BOLT_AUTH_URL: ${LAMASSU_BOLT_AUTH_URL:?'missing/empty env var $LAMASSU_BOLT_AUTH_URL'}
      LAMASSU_BOLT_CLIENT_ID: ${LAMASSU_BOLT_CLIENT_ID:?'missing/empty env var $LAMASSU_BOLT_CLIENT_ID'}
      LAMASSU_BOLT_CLIENT_PASSWORD: ${LAMASSU_BOLT_CLIENT_PASSWORD:?'missing/empty env var $LAMASSU_BOLT_CLIENT_PASSWORD'}
      # depending on base image, JAVA_TOOL_OPTIONS must be used
      LAMASSU_BASE_URL: ${LAMASSU_BASE_URL}
      JDK_JAVA_OPTIONS:
        -Dspring.config.location=/etc/application-config/application.properties
        -Dspring.profiles.active=leader
        -Dorg.entur.lamassu.adminPassword=${LAMASSU_ADMIN_PASSWORD}
        -Dhttp.proxyHost=transformer-proxy
        -Dhttp.proxyPort=8080
    volumes:
      - ./etc/lamassu/:/etc/application-config/
      - ./var/gbfs/feeds:/var/gbfs/feeds
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.lamassu.rule=PathPrefix(`/sharing`)"
      - "traefik.http.routers.lamassu.middlewares=sharing-stripprefix"
      - "traefik.http.middlewares.sharing-stripprefix.stripprefix.prefixes=/sharing"
      - "traefik.http.services.lamassu.loadbalancer.server.port=8080"
      # Note: /admin is shifted to the management port in application.properties'
      # management.endpoints.web.exposure.include (currently including admin,info,health,prometheus)
    depends_on:
        redis:
          condition: service_healthy
        transformer-proxy:
          condition: service_started
    restart: unless-stopped

  redis:
    networks: [ipl]
    image: redis:6-alpine
    # Redis tries to use all memory it gets, and doesn't query the container's memory limit.
    # see also https://stackoverflow.com/a/70779280
    # --save '' should explicitly disable saving to volume data, avoiding no space left on devide errors
    command: --maxmemory ${REDIS_MEMORY_MB:-256}mb --save ''
    deploy:
      resources:
        limits:
          memory: ${REDIS_MEMORY_MB:-256}M
    # The redis image doesn't provide a health check by itself.
    # see also https://github.com/docker-library/redis/issues/91
    healthcheck:
      test: "redis-cli --raw incr ping"
      interval: 1s
      timeout: 3s
      retries: 20
    restart: unless-stopped

  ipl-db:
    networks: [ipl]
    image: ${IPL_POSTGIS_IMAGE}
    volumes:
      - ./var/ipl-db/data:/var/lib/postgresql/data
    ports:
      - ${IPL_POSTGRES_PORT}:5432
    environment:
      - POSTGRES_DB=${IPL_POSTGRES_DB}
      - POSTGRES_USER=${IPL_POSTGRES_USER}
      - POSTGRES_PASSWORD=${IPL_POSTGRES_PASSWORD:?missing/empty $$IPL_POSTGRES_PASSWORD}
    healthcheck:
      test: "PGPASSWORD=${IPL_POSTGRES_PASSWORD:?missing/empty $$IPL_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${IPL_POSTGRES_USER} -d ${IPL_POSTGRES_DB}"
      interval: 5s
      timeout: 3s
      retries: 12
    restart: unless-stopped

  geoserver:
    networks: [ipl]
    image: ${GEOSERVER_IMAGE}
    user: *local-user
    volumes:
      # var/geoserver defaults to geoserver/data_dir, custom config shall explicitly be defined in etc/geoserver
      - ./var/geoserver/datadir/:/var/geoserver/datadir/
      - ./var/geoserver/logs/:/var/geoserver/logs/
      - ./var/geoserver/gwc_cache_dir/:/var/geoserver/gwc_cache_dir/
      - ./etc/geoserver/:/var/geoserver/datadir_template/
      # global.xml is written by geoserver before config reload, so we need to have it mounted already at start
      - ./etc/geoserver/global.xml:/var/geoserver/datadir/global.xml
      - ./bin/geoserver/geoserver-rest-config.sh:/usr/local/bin/geoserver-rest-config.sh
      - ./bin/geoserver/geoserver-rest-reload.sh:/usr/local/bin/geoserver-rest-reload.sh
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.geoserver.rule=PathPrefix(`/geoserver`)"
      - "traefik.http.routers.geoserver.middlewares=geoserver-middleware"
      - "traefik.http.middlewares.geoserver-middleware.headers.accesscontrolallowmethods=GET,OPTIONS,HEAD"
      - "traefik.http.middlewares.geoserver-middleware.headers.accesscontrolallowheaders=*"
      - "traefik.http.services.geoserver.loadbalancer.server.port=8080"
    ports:
      - ${GEOSERVER_PORT}:8080
    environment:
      # Special env vars picked up by the kartoza/geoserver Docker image, getting passed in as Java system properties (`-D`).
      - ADMIN_PASSWORD=${GEOSERVER_ADMIN_PASSWORD}
      - INITIAL_MEMORY=${GEOSERVER_INITIAL_MEMORY}
      - MAXIMUM_MEMORY=${GEOSERVER_MAXIMUM_MEMORY}
      - GEOSERVER_DATA_DIR_CUSTOM=/var/geoserver/datadir_template/
      - GEOSERVER_LOGGING_PROFILE=${GEOSERVER_LOGGING_PROFILE}
      - GEOSERVER_CSRF_WHITELIST=${GEOSERVER_CSRF_WHITELIST:?missing/empty $$GEOSERVER_CSRF_WHITELIST}
      - GEOSERVER_PROXY_BASE_URL=${GEOSERVER_PROXY_BASE_URL:?missing/empty $$GEOSERVER_PROXY_BASE_URL}
      # All we want to do is set user.timezone and DALLOW_ENV_PARAMETRIZATION. Due to https://github.com/geosolutions-it/docker-geoserver/issues/133
      # we currently need to pass in all default properties already defined in the dockerfile
      - JAVA_OPTS= >-
          -Duser.timezone=${TZ:?'missing/empty $TZ'}
          -DALLOW_ENV_PARAMETRIZATION=true
          -Xms$${INITIAL_MEMORY}
          -Xmx$${MAXIMUM_MEMORY}
          -Djava.awt.headless=true
          -server
          -Dfile.encoding=UTF8
          -Djavax.servlet.request.encoding=UTF-8
          -Djavax.servlet.response.encoding=UTF-8
          -XX:SoftRefLRUPolicyMSPerMB=36000
          -XX:+UseG1GC
          -XX:MaxGCPauseMillis=200
          -XX:ParallelGCThreads=20
          -XX:ConcGCThreads=5
          -Dorg.geotools.coverage.jaiext.enabled=$${JAIEXT_ENABLED}
          -Dorg.geotools.shapefile.datetime=true
          -DGEOSERVER_LOG_LOCATION=$${GEOSERVER_LOG_LOCATION}
          -DGEOWEBCACHE_CONFIG_DIR=$${GEOWEBCACHE_CONFIG_DIR}
          -DGEOWEBCACHE_CACHE_DIR=$${GEOWEBCACHE_CACHE_DIR}
          -DNETCDF_DATA_DIR=$${NETCDF_DATA_DIR}
          -DGRIB_CACHE_DIR=$${GRIB_CACHE_DIR}
      # The following parameters are *not* picked up by geosoultionsit/geoserver, but instead passed through as "regular" env vars, and then read by Geoserver whenever one of the config files references them.
      - PGBOUNCER_POSTGRES_USER=${PGBOUNCER_POSTGRES_USER:?missing/empty $$PGBOUNCER_POSTGRES_USER}
      - PGBOUNCER_POSTGRES_PASSWORD=${PGBOUNCER_POSTGRES_PASSWORD:?missing/empty $$PGBOUNCER_POSTGRES_PASSWORD}
      - PARK_API_POSTGRES_PASSWORD=${PARK_API_POSTGRES_PASSWORD?missing/empty $$PARK_API_POSTGRES_PASSWORD}
    depends_on:
      pgbouncer:
        # For sharing & transit layers
        condition: service_healthy
      park-api-db:
        # For parking_sites layer
        condition: service_healthy
    healthcheck:
      test: "curl -fsS -o /dev/null -u 'admin:${GEOSERVER_ADMIN_PASSWORD}' http://localhost:8080/geoserver/rest/about/version.xml"
      interval: 0m10s
      timeout: 10s
      retries: 22
    restart: unless-stopped

  # This service runs the postgres DB used by dagster for run storage, schedule storage,
  # and event log storage.
  dagster-postgresql:
    networks: [ipl]
    image: ${DAGSTER_POSTGRES_IMAGE}
    environment:
      POSTGRES_USER: "${DAGSTER_POSTGRES_USER}"
      POSTGRES_PASSWORD: "${DAGSTER_POSTGRES_PASSWORD}"
      POSTGRES_DB: "${DAGSTER_POSTGRES_DB}"
    volumes:
        - ./var/dagster-postgresql/data:/var/lib/postgresql/data
    healthcheck:
        test: "PGPASSWORD=${DAGSTER_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${DAGSTER_POSTGRES_USER} -d ${DAGSTER_POSTGRES_DB}"
        interval: 5s
        timeout: 3s
        retries: 12
    restart: unless-stopped

  # This service runs the gRPC server that loads your user code, in both dagster-webserver
  # and dagster-daemon. Multiple containers like this can be deployed separately - each just needs to run on
  # its own port, and have its own entry in the workspace.yaml file that's loaded by
  # dagster-webserver.
  dagster-pipeline:
    networks: [ipl]
    image: ${DAGSTER_PIPELINE_IMAGE}
    environment:
      # connect via pgbouncer to improve performance
      - PGHOST=pgbouncer
      - PGPORT=${PGBOUNCER_POSTGRES_PORT:?missing/empty $PGBOUNCER_POSTGRES_PORT}
      - PGUSER=${PGBOUNCER_POSTGRES_USER:?missing/empty $PGBOUNCER_POSTGRES_USER}
      - PGPASSWORD=${PGBOUNCER_POSTGRES_PASSWORD:?missing/empty $PGBOUNCER_POSTGRES_PASSWORD}
      - PGDATABASE=dagster # determined by pgbouncer!
      # Use docker hostname of lamassu service to avoid roundtrip via proxy service
      # todo [breaking]: remove `IPL_LAMASSU_BASE_URL` to prevent confusion with `LAMASSU_BASE_URL`
      # temporarilly remove trainling slash as pipeline adds another / which causes the spring firewall to block
      - IPL_LAMASSU_BASE_URL=http://lamassu:8080
      - IPL_LAMASSU_INTERNAL_BASE_URL=http://lamassu:8080/
      - IPL_POSTGRES_HOST=ipl-db
      - IPL_POSTGRES_PORT
      - IPL_POSTGRES_DB
      - IPL_POSTGRES_USER
      - IPL_POSTGRES_PASSWORD
      # Variables required for GTFS-Import
      - IPL_GTFS_IMPORTER_IMAGE
      - IPL_GTFS_IMPORTER_NETWORK
      - IPL_GTFS_IMPORTER_GTFS_DOWNLOAD_URL
      - IPL_GTFS_IMPORTER_GTFS_DOWNLOAD_USER_AGENT
      - IPL_GTFS_DB_POSTGRES_HOST
      - IPL_GTFS_DB_POSTGRES_USER
      - IPL_GTFS_DB_POSTGRES_PASSWORD
      - IPL_GTFS_DB_POSTGRES_DB
      - IPL_GTFS_DB_POSTGRES_DB_PREFIX
      - IPL_GTFS_DB_POSTGREST_USER
      - IPL_GTFS_DB_POSTGREST_PASSWORD
      # Required to mount volumes for docker-in-docker executions
      - IPL_GTFS_IMPORTER_HOST_CUSTOM_SCRIPTS_DIR=${PWD}/etc/gtfs/
      - IPL_GTFS_IMPORTER_HOST_GTFS_OUTPUT_DIR=${PWD}/var/gtfs/
    volumes:
      # Make docker API accessible so we can start containers in jobs
      - /var/run/docker.sock:/var/run/docker.sock
      # mount storage dir so logs/assets are written to volume shared with dagaster-daemon/dagit
      - ./var/dagster/storage/:/opt/dagster/dagster_home/storage/
    depends_on:
        pgbouncer:
          condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "/bin/bash", "printf", "'GET / HTTP/1.1\n\n'", ">/dev/tcp/127.0.0.1/4000"]
      timeout: 1s
      interval: 1s
      start_period: 2s
      retries: 15
  # This service runs dagster-webserver, which loads your user code from the user code container.
  # Since our instance uses the QueuedRunCoordinator, any runs submitted from dagster-webserver
  # will be put on a queue and later dequeued and launched by dagster-daemon.
  dagster-dagit:
    networks: [ipl]
    image: ${DAGSTER_DAGIT_IMAGE}
    # expose:
    #   - "3000"
    ports:
      - "3000:3000"
    environment:
      # connect via pgbouncer to improve performance
      PGHOST: pgbouncer
      PGPORT: ${PGBOUNCER_POSTGRES_PORT:?missing/empty $PGBOUNCER_POSTGRES_PORT}
      PGUSER: ${PGBOUNCER_POSTGRES_USER:?missing/empty $PGBOUNCER_POSTGRES_USER}
      PGPASSWORD: ${PGBOUNCER_POSTGRES_PASSWORD:?missing/empty $PGBOUNCER_POSTGRES_PASSWORD}
      PGDATABASE: dagster # determined by pgbouncer!
    volumes: # Make docker API accessible so we can terminate containers from dagster-webserver
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - ./etc/dagster:/opt/dagster/dagster_home/
      - ./var/dagster/storage/:/opt/dagster/dagster_home/storage/
    depends_on:
      pgbouncer:
        condition: service_healthy
      dagster-pipeline:
        condition: service_started
    restart: unless-stopped

  # This service runs the dagster-daemon process, which is responsible for taking runs
  # off of the queue and launching them, as well as creating runs from schedules or sensors.
  dagster-daemon:
    networks: [ipl]
    image: ${DAGSTER_DAEMON_IMAGE}
    environment:
      # connect via pgbouncer to improve performance
      PGHOST: pgbouncer
      PGPORT: ${PGBOUNCER_POSTGRES_PORT:?missing/empty $PGBOUNCER_POSTGRES_PORT}
      PGUSER: ${PGBOUNCER_POSTGRES_USER:?missing/empty $PGBOUNCER_POSTGRES_USER}
      PGPASSWORD: ${PGBOUNCER_POSTGRES_PASSWORD:?missing/empty $PGBOUNCER_POSTGRES_PASSWORD}
      PGDATABASE: dagster # determined by pgbouncer!
    volumes: # Make docker API accessible so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - ./etc/dagster:/opt/dagster/dagster_home/
      - ./var/dagster/storage/:/opt/dagster/dagster_home/storage/
    depends_on:
      pgbouncer:
        condition: service_healthy
      dagster-pipeline:
        condition: service_started
    restart: unless-stopped

  # This service stores GTFS data imported by gtfs-importer.
  gtfs-db:
    networks: [ipl]
    image: ${IPL_GTFS_DB_IMAGE}
    volumes:
      - ./var/gtfs/gtfs-db:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: ${IPL_GTFS_DB_POSTGRES_USER}
      POSTGRES_PASSWORD: ${IPL_GTFS_DB_POSTGRES_PASSWORD}
      POSTGRES_DB: ${IPL_GTFS_DB_POSTGRES_DB}
    healthcheck:
      test: "PGPASSWORD=${IPL_GTFS_DB_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${IPL_GTFS_DB_POSTGRES_USER} -d ${IPL_GTFS_DB_POSTGRES_DB}"
      timeout: 3s
      interval: 5s
      start_period: 10s
      retries: 10
    restart: unless-stopped

  # pgbouncer acts like a facade in front of PostgreSQL instances (ipl-db, gtfs-db, dagster-postgresql), providing
  # - programmatically configurable query routing (used by `make import-new-gtfs`)
  # - improved connect performance for clients, especially Dagster workers
  pgbouncer:
    networks: [ipl]
    depends_on:
      ipl-db:
        condition: service_started
      gtfs-db:
        condition: service_started
      dagster-postgresql:
        condition: service_started
    links:
      - ipl-db
      - gtfs-db
      - dagster-postgresql
    image: ${PGBOUNCER_IMAGE}
    ports:
      - ${PGBOUNCER_POSTGRES_PORT:?missing/empty $PGBOUNCER_POSTGRES_PORT}:6432
    volumes:
      # contains the latest import's DB name
      - ./var/gtfs/pgbouncer-dsn.txt:/var/gtfs-pgbouncer-dsn.txt
      - ./etc/reload-pgbouncer-databases.sh:/reload-pgbouncer-databases.sh
    environment:
      # Even if we define all upstream database connections "manually" using $PGBOUNCER_DSN_* (see below), the bitnami/pgbouncer Docker image expects 1 connection to be specified via $POSTGRESQL_*, which it implicitly adds to the `[database]` section of pgbouncer.ini. Note that the specified database credentials must be valid.
      # However, the $POSTGRESQL_* env vars are *also* used to configure client access to pgbouncer: They get implicitly added to the generated userlist.txt, meaning that clients *must* use them (if there are no other user/password pairs defined, which we don't do) to connect to *any* exposed connection.
      # Therefore, we duplicate the connection to ipl-db and expose it as `meta`, clearly denoting it as a connection *not* intended for pgbouncer clients.
      PGBOUNCER_DATABASE: meta
      POSTGRESQL_HOST: ipl-db
      POSTGRESQL_DATABASE: ${IPL_POSTGRES_DB:?missing/empty $IPL_POSTGRES_DB}
      POSTGRESQL_USERNAME: ${PGBOUNCER_POSTGRES_USER:?missing/empty $PGBOUNCER_POSTGRES_USER}
      POSTGRESQL_PASSWORD: ${PGBOUNCER_POSTGRES_PASSWORD:?missing/empty $PGBOUNCER_POSTGRES_PASSWORD}
      # > Most polite method. When a client connects, a server connection will be assigned to it for the whole duration the client stays connected. When the client disconnects, the server connection will be put back into the pool. This is the default method.
      # We hardcode this, because we definitely don't want the other pooling modes.
      PGBOUNCER_POOL_MODE: session
      # todo: remove? should work without 🤔
      # see also https://github.com/bitnami/containers/issues/48636#issuecomment-1722518107
      PGBOUNCER_AUTH_TYPE: md5
      # Geoserver connects with `extra_float_digits` set.
      # see https://github.com/bitnami/containers/issues/25394#issuecomment-1457893192
      # dagster-* connect with `statement_timeout` set.
      PGBOUNCER_IGNORE_STARTUP_PARAMETERS: extra_float_digits, statement_timeout

      # upsteam connections
      # see also https://www.pgbouncer.org/config.html#section-databases
      # see also https://www.postgresql.org/docs/15/libpq-connect.html#id-1.7.3.8.3.5
      # expose ipl-db as `ipl`
      PGBOUNCER_DSN_0: "ipl=host=ipl-db dbname=${IPL_POSTGRES_DB:?missing/empty $IPL_POSTGRES_DB} user=${IPL_POSTGRES_USER:?missing/empty $IPL_POSTGRES_USER} password=${IPL_POSTGRES_PASSWORD:?missing/empty $IPL_POSTGRES_PASSWORD}"
      # expose dagster-postgresql as `dagster`
      PGBOUNCER_DSN_1: "dagster=host=dagster-postgresql dbname=${DAGSTER_POSTGRES_DB:?missing/empty $DAGSTER_POSTGRES_DB} user=${DAGSTER_POSTGRES_USER:?missing/empty $DAGSTER_POSTGRES_USER} password=${DAGSTER_POSTGRES_PASSWORD:?missing/empty $DAGSTER_POSTGRES_PASSWORD}"
      # expose the latest GTFS import within gtfs-db as `gtfs`
      # $PGBOUNCER_DSN_2 gets generated from /var/gtfs-pgbouncer-dsn.txt (which is written by gtfs-importer) by the Docker image's entrypoint script.
      # see https://github.com/bitnami/containers/issues/46152#issuecomment-1695320501
      # todo: not yet, push & PR our changes, see `image` field above!
      PGBOUNCER_DSN_2_FILE: /var/gtfs-pgbouncer-dsn.txt
    restart: unless-stopped
    healthcheck:
      # pgbouncer exposes a `pgbouncer` "meta database", providing an interface for statistics and to administer the instance. We use it here to check if pgbouncer is working properly.
      test: 'env PGPASSWORD="$$POSTGRESQL_PASSWORD" psql -p 6432 -U "$$POSTGRESQL_USERNAME" pgbouncer -b -c "SHOW USERS" >/dev/null'
      interval: 0m15s
      timeout: 5s
      retries: 10

  # todo /arrivals_departures: enforce stop_id filtering?
  gtfs-api:
    networks: [ipl]
    depends_on:
      pgbouncer:
        condition: service_healthy
    links:
      - pgbouncer
    image: postgrest/postgrest
    ports:
      - ${IPL_GTFS_API_PORT}:3000
    read_only: true
    environment:
      # connect via pgbouncer to regardless of the GTFS DBs' suffixes & improve performance
      PGHOST: pgbouncer
      PGPORT: ${PGBOUNCER_POSTGRES_PORT:?missing/empty $PGBOUNCER_POSTGRES_PORT}
      PGUSER: ${PGBOUNCER_POSTGRES_USER:?missing/empty $PGBOUNCER_POSTGRES_USER}
      PGPASSWORD: ${PGBOUNCER_POSTGRES_PASSWORD:?missing/empty $PGBOUNCER_POSTGRES_PASSWORD}
      PGDATABASE: gtfs # determined by pgbouncer!
      # PostgREST-specific env vars
      PGRST_OPENAPI_SERVER_PROXY_URI: ${IPL_GTFS_API_PUBLIC_BASE_URL}
      PGRST_DB_SCHEMAS: api
      PGRST_DB_ANON_ROLE: web_anon
      PGRST_DB_MAX_ROWS: ${IPL_GTFS_API_MAX_ROWS:?missing/empty $IPL_GTFS_API_MAX_ROWS}
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.gtfs-api.rule=PathPrefix(`/gtfs`)"
      - "traefik.http.routers.gtfs-api.middlewares=gtfs-stripprefix"
      - "traefik.http.middlewares.gtfs-stripprefix.stripprefix.prefixes=/gtfs"
      - "traefik.http.services.gtfs-api.loadbalancer.server.port=3000"
    restart: unless-stopped

  gtfs-importer:
    networks: [ipl]
    # gtfstidy currently only provides a linux/amd64 binary
    # see also https://github.com/patrickbr/gtfstidy/pull/21
    platform: linux/amd64
    # gtfs-imports will be performed via dagster nevertheless, we keep it here for now
    # i.e. to have it downloaded
    image: ${IPL_GTFS_IMPORTER_IMAGE:?missing/empty $$IPL_GTFS_IMPORTER_IMAGE}
    profiles:
      - import-new-gtfs
    depends_on:
      gtfs-db:
        condition: service_healthy
    links:
      - gtfs-db
    volumes:
      - ./var/gtfs:/var/gtfs
      - ./etc/gtfs:/etc/gtfs
    environment:
      PGHOST: gtfs-db
      PGUSER: ${IPL_GTFS_DB_POSTGRES_USER:?missing/empty $$IPL_GTFS_DB_POSTGRES_USER}
      PGPASSWORD: ${IPL_GTFS_DB_POSTGRES_PASSWORD:?missing/empty $$IPL_GTFS_DB_POSTGRES_PASSWORD}
      # the default DB, used by gtfs-importer to keep track of imports
      PGDATABASE: ${IPL_GTFS_DB_POSTGRES_DB:?missing/empty $$IPL_GTFS_DB_POSTGRES_DB}
      GTFS_DOWNLOAD_URL: ${IPL_GTFS_IMPORTER_GTFS_DOWNLOAD_URL:?missing/empty $$IPL_GTFS_IMPORTER_GTFS_DOWNLOAD_URL}
      GTFS_DOWNLOAD_USER_AGENT: ${IPL_GTFS_IMPORTER_GTFS_DOWNLOAD_USER_AGENT:?missing/empty $$IPL_GTFS_IMPORTER_GTFS_DOWNLOAD_USER_AGENT}
      # the prefix of all DBs created for imports by gtfs-importer
      GTFS_IMPORTER_DB_PREFIX: ${IPL_GTFS_DB_POSTGRES_DB_PREFIX:?missing/empty $$IPL_GTFS_DB_POSTGRES_DB_PREFIX}
      # path to the file containing the latest import's DB name
      GTFS_IMPORTER_DSN_FILE: /var/gtfs/pgbouncer-dsn.txt
      GTFS_TMP_DIR: /var/gtfs
      POSTGREST_USER: ${IPL_GTFS_DB_POSTGREST_USER:?missing/empty $$IPL_GTFS_DB_POSTGREST_USER}
      POSTGREST_PASSWORD: ${IPL_GTFS_DB_POSTGREST_PASSWORD:?missing/empty $$IPL_GTFS_DB_POSTGREST_PASSWORD}

  gtfs-swagger-ui:
    networks: [ipl]
    depends_on:
      gtfs-api:
        condition: service_started
    image: swaggerapi/swagger-ui
    ports:
      - ${IPL_GTFS_SWAGGER_UI_PORT}:8080
    environment:
      # Swagger UI will call gtfs-api in the browser, so we need to use its "outside" URL here.
      API_URL: ${IPL_GTFS_API_PUBLIC_BASE_URL}
      BASE_URL: /docs/gtfs
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.gtfs-swagger-ui.rule=PathPrefix(`/docs/gtfs`)"
      - "traefik.http.services.gtfs-swagger-ui.loadbalancer.server.port=8080"
    logging:
      # disable stdout/stderr logging entirely
      driver: none
    restart: unless-stopped

  ocpdb-flask:
    <<: *ocpdb-defaults
    command: ["gunicorn", "--bind", "0.0.0.0:5000", "webapp.entry_point_gunicorn:app"]
    restart: unless-stopped
    ports:
      - ${OCPDB_API_PORT}:5000
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ocpdb-flask.rule=PathPrefix(`/ocpdb`)"
      - "traefik.http.services.ocpdb-flask.loadbalancer.server.port=5000"
      - "traefik.http.routers.ocpdb-flask.middlewares=ocpdb-stripprefix"
      - "traefik.http.middlewares.ocpdb-stripprefix.stripprefix.prefixes=/ocpdb"

  ocpdb-worker:
    <<: *ocpdb-defaults
    restart: unless-stopped
    command: ["celery", "-A", "webapp.entry_point_celery", "worker", "-c", "2"]

  ocpdb-init:
    <<: *ocpdb-defaults
    command: ["sh", "-c", "flask db upgrade && flask bnetza import-web"]

  ocpdb-db:
    networks: [ipl]
    image: ${OCPDB_DB_IMAGE}
    volumes:
      - ./var/ocpdb/ocpdb-db:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${OCPDB_POSTGRES_USER}
      - POSTGRES_PASSWORD=${OCPDB_POSTGRES_PASSWORD}
      - POSTGRES_DB=${OCPDB_POSTGRES_DB}
    healthcheck:
      test: "PGPASSWORD=${OCPDB_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${OCPDB_POSTGRES_USER} -d ${OCPDB_POSTGRES_DB}"
      interval: 10s
      timeout: 3s
      retries: 3
    restart: unless-stopped

  ocpdb-rabbitmq:
    <<: *generic-rabbitmq

  park-api-flask:
    <<: *park-api-defaults
    restart: unless-stopped
    command: ["gunicorn", "--bind", "0.0.0.0:5000", "webapp.entry_point_gunicorn:app"]
    ports:
      - ${PARK_API_API_PORT}:5000
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.park-api-flask.rule=PathPrefix(`/park-api`)"
      - "traefik.http.services.park-api-flask.loadbalancer.server.port=5000"
      - "traefik.http.routers.park-api-flask.middlewares=park-api-stripprefix"
      - "traefik.http.middlewares.park-api-stripprefix.stripprefix.prefixes=/park-api"

  park-api-init:
    <<: *park-api-defaults
    command: ["bash", "-c", "flask db upgrade && flask source init-converters"]

  park-api-worker:
    <<: *park-api-defaults
    restart: unless-stopped
    command: ["celery", "-A", "webapp.entry_point_celery", "worker", "-c", "2"]

  park-api-heartbeat:
    <<: *park-api-defaults
    restart: unless-stopped
    command: ["celery", "-A", "webapp.entry_point_celery", "beat", "-s", "/tmp/celerybeat-schedule"]

  park-api-db:
    networks: [ipl]
    image: ${PARK_API_DB_IMAGE}
    volumes:
      - ./var/park-api/park-api-db:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${PARK_API_POSTGRES_USER}
      - POSTGRES_PASSWORD=${PARK_API_POSTGRES_PASSWORD}
      - POSTGRES_DB=${PARK_API_POSTGRES_DB}
    healthcheck:
      test: "PGPASSWORD=${PARK_API_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${PARK_API_POSTGRES_USER} -d ${PARK_API_POSTGRES_DB}"
      interval: 10s
      timeout: 3s
      retries: 3
    restart: unless-stopped

  park-api-rabbitmq:
    <<: *generic-rabbitmq

networks:
  ipl:
    driver: bridge
    ipam:
      driver: default
      config:
      - subnet: 172.28.0.0/16
        ip_range: 172.28.5.0/24
        # Traefikforwards X-Forwarded-* headers only from trusted IPs, so we need a specific IP here.
        gateway: 172.28.5.254
