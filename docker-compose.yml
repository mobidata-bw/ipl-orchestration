# compose "namespace", prepended to all network names, container names, etc.
name: ipl

services:
  # Traefik is our ingress: requests enter our infrastructure through it.
  # It is an HTTP reverse proxy, discovering available services (and their containers)
  # automatically by using the Docker API.
  ingress:
    networks: [ipl]
    image: traefik:v2.10
    ports:
      # todo: change to 80:80
      - "8080:80"
      # # todo: change to 443:443
      # - "8443:443"
      # todo: change to 8080:8080
      # web UI, Prometheus, etc.
      - "8081:8080"
    volumes:
      - ./etc/traefik:/etc/traefik
      - ./var/log/traefik:/var/log/traefik
      # So that Traefik can listen to the Docker events.
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: "traefik healthcheck"

  # This service applies a couple of conversions for certain GBFS feeds to fix
  # syntax issues which prevent importing in Lamassu.
  # It needs to be configured as proxy for Lamassu and feeds to be filtered should
  # explicitly be requests using http. Proxy will take care of https requesting and
  # necessary response rewrites.
  transformer-proxy:
    networks: [ipl]
    image: ${TRANSFORMER_PROXY_IMAGE}
    volumes:
      # Mount config file which declares hosts to proxy
      - ./etc/transformer-proxy/config.yaml:/app/config.yaml
    ports:
      # We export the port for debugging purposes. Lamassu acceses via docker network
      - ${TRANSFORMER_PROXY_PORT}:8080

  # This service requests non-standard sharing provider APIs and generates
  # a gbfs feed intended for local import into Lamassu. The volume needs to be
  # mounted by the lamassu container also.
  x2gbfs:
    networks: [ipl]
    image: ${X2GBFS_IMAGE}
    volumes:
      - ./var/gbfs/feeds:/app/out/
    command: -p deer -b file:///var/gbfs/feeds -i 45
    environment:
      - DEER_API_URL
      - DEER_USER
      - DEER_PASSWORD

  lamassu:
    networks: [ipl]
    image: ${LAMASSU_IMAGE}
    ports:
      - ${LAMASSU_PORT}:8080
      - ${LAMASSU_ADMIN_PORT}:9001
    environment:
      LAMASSU_DB_CONNECT_CLIENT_ID: ${LAMASSU_DB_CONNECT_CLIENT_ID:?'missing/empty env var $LAMASSU_DB_CONNECT_CLIENT_ID'}
      LAMASSU_DB_CONNECT_API_KEY: ${LAMASSU_DB_CONNECT_API_KEY:?'missing/empty env var $LAMASSU_DB_CONNECT_API_KEY'}
      # depending on base image, JAVA_TOOL_OPTIONS must be used
      LAMASSU_BASE_URL: ${LAMASSU_BASE_URL}
      JDK_JAVA_OPTIONS:
        -Dspring.config.location=/etc/application-config/application.properties
        -Dspring.profiles.active=leader
        -Dorg.entur.lamassu.adminPassword=${LAMASSU_ADMIN_PASSWORD}
        -Dhttp.proxyHost=transformer-proxy
        -Dhttp.proxyPort=8080
    volumes:
      - ./etc/lamassu/:/etc/application-config/
      - ./var/gbfs/feeds:/var/gbfs/feeds
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.lamassu.rule=PathPrefix(`/sharing`)"
      - "traefik.http.routers.lamassu.middlewares=sharing-stripprefix"
      - "traefik.http.middlewares.sharing-stripprefix.stripprefix.prefixes=/sharing"
      - "traefik.http.services.lamassu.loadbalancer.server.port=8080"
      # Note: /admin is shifted to the management port in application.properties'
      # management.endpoints.web.exposure.include (currently including admin,info,health,prometheus)
    depends_on:
        redis:
          condition: service_healthy
        transformer-proxy:
          condition: service_started

  redis:
    networks: [ipl]
    image: redis:6-alpine
    # Redis tries to use all memory it gets, and doesn't query the container's memory limit.
    # see also https://stackoverflow.com/a/70779280
    # --save '' should explicitly disable saving to volume data, avoiding no space left on devide errors
    command: --maxmemory ${REDIS_MEMORY_MB:-256}mb --save ''
    deploy:
      resources:
        limits:
          memory: ${REDIS_MEMORY_MB:-256}M
    # The redis image doesn't provide a health check by itself.
    # see also https://github.com/docker-library/redis/issues/91
    healthcheck:
      test: "redis-cli --raw incr ping"

  ipl-db:
    networks: [ipl]
    image: ${IPL_POSTGIS_IMAGE}
    volumes:
      - ./var/ipl-db/data:/var/lib/postgresql/data
    ports:
      - ${IPL_POSTGRES_PORT}:5432
    environment:
      - POSTGRES_DB=${IPL_POSTGRES_DB}
      - POSTGRES_USER=${IPL_POSTGRES_USER}
      - POSTGRES_PASSWORD=${IPL_POSTGRES_PASSWORD}
    restart: on-failure
    healthcheck:
      test: "PGPASSWORD=${IPL_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${IPL_POSTGRES_USER} -d ${IPL_POSTGRES_DB}"

  geoserver:
    networks: [ipl]
    image: ${GEOSERVER_IMAGE}
    volumes:
      # var/geoserver defaults to geoserver/data_dir, custom config shall explicitly be defined in etc/geoserver
      - ./var/geoserver/:/opt/geoserver/data_dir/
      - ./etc/geoserver/global.xml:/opt/geoserver/data_dir/global.xml
      - ./etc/geoserver/logging.xml:/opt/geoserver/data_dir/logging.xml
      - ./etc/geoserver/workspaces:/opt/geoserver/data_dir/workspaces
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.geoserver.rule=PathPrefix(`/geoserver`)"
      - "traefik.http.services.geoserver.loadbalancer.server.port=8080"
    ports:
      - ${GEOSERVER_PORT}:8080
    restart: on-failure
    environment:
      # Special env vars picked up by the kartoza/geoserver Docker image, getting passed in as Java system properties (`-D`).
      - GEOSERVER_ADMIN_PASSWORD=${GEOSERVER_ADMIN_PASSWORD}
      - GEOSERVER_ADMIN_USER=${GEOSERVER_ADMIN_USER}
      - INITIAL_MEMORY=${GEOSERVER_INITIAL_MEMORY}
      - MAXIMUM_MEMORY=${GEOSERVER_MAXIMUM_MEMORY}
      - GEOSERVER_LOGGING_PROFILE=${GEOSERVER_LOGGING_PROFILE}
      # to provide env variables, see https://docs.geoserver.org/stable/en/user/datadirectory/configtemplate.html
      # kartoza/geoserver maps PROXY_BASE_URL_PARAMETRIZATION to ALLOW_ENV_PARAMETRIZATION, see https://github.com/kartoza/docker-geoserver/blob/844c7a26acd1687358c821ea73117a721f25f7b6/scripts/entrypoint.sh#L72
      - PROXY_BASE_URL_PARAMETRIZATION=true
      # The following parameters are *not* picked up by kartoza/geoserver, but instead passed through as "regular" env vars, and then read by Geoserver whenever one of the config files references them.
      - IPL_POSTGRES_PASSWORD=${IPL_POSTGRES_PASSWORD}
      - IPL_GTFS_DB_POSTGRES_PASSWORD=${IPL_GTFS_DB_POSTGRES_PASSWORD}
    # contains the latest import's DB name as `PGDATABASE`
    env_file: ./.imported-gtfs-db.env
    depends_on:
      ipl-db:
        # For sharing layers
        condition: service_healthy
      gtfs-db:
        # For transit layer
        condition: service_healthy     
    healthcheck:
      test: "curl -fsS -o /dev/null -u '${GEOSERVER_ADMIN_USER}':'${GEOSERVER_ADMIN_PASSWORD}' http://localhost:8080/geoserver/rest/about/version.xml"
      interval: 0m15s
      timeout: 10s
      retries: 15

  # This service runs the postgres DB used by dagster for run storage, schedule storage,
  # and event log storage.
  dagster-postgresql:
    networks: [ipl]
    image: ${DAGSTER_POSTGRES_IMAGE}
    environment:
      POSTGRES_USER: "${DAGSTER_POSTGRES_USER}"
      POSTGRES_PASSWORD: "${DAGSTER_POSTGRES_PASSWORD}"
      POSTGRES_DB: "${DAGSTER_POSTGRES_DB}"
    restart: on-failure
    volumes:
        - ./var/dagster-postgresql/data:/var/lib/postgresql/data
    healthcheck:
        test: "PGPASSWORD=${DAGSTER_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${DAGSTER_POSTGRES_USER} -d ${DAGSTER_POSTGRES_DB}"
        interval: 5s
        timeout: 3s
        retries: 12

  # This service runs the gRPC server that loads your user code, in both dagster-webserver
  # and dagster-daemon. Multiple containers like this can be deployed separately - each just needs to run on
  # its own port, and have its own entry in the workspace.yaml file that's loaded by
  # dagster-webserver.
  dagster-pipeline:
    networks: [ipl]
    image: ${DAGSTER_PIPELINE_IMAGE}
    restart: always
    environment:
      - PGHOST=dagster-postgresql
      - PGUSER=${DAGSTER_POSTGRES_USER}
      - PGPASSWORD=${DAGSTER_POSTGRES_PASSWORD}
      - PGDATABASE=${DAGSTER_POSTGRES_DB}
      # Use docker hostname of lamassu service to avoid roundtrip via proxy service
      - IPL_LAMASSU_BASE_URL=http://lamassu/
      - IPL_POSTGRES_HOST=ipl-db
      - IPL_POSTGRES_PORT
      - IPL_POSTGRES_DB
      - IPL_POSTGRES_USER
      - IPL_POSTGRES_PASSWORD
    depends_on:
        dagster-postgresql:
          condition: service_healthy

  # This service runs dagster-webserver, which loads your user code from the user code container.
  # Since our instance uses the QueuedRunCoordinator, any runs submitted from dagster-webserver
  # will be put on a queue and later dequeued and launched by dagster-daemon.
  dagster-dagit:
    networks: [ipl]
    image: ${DAGSTER_DAGIT_IMAGE}
    # expose:
    #   - "3000"
    ports:
      - "3000:3000"
    environment:
      PGHOST: dagster-postgresql
      PGUSER: ${DAGSTER_POSTGRES_USER}
      PGPASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      PGDATABASE: ${DAGSTER_POSTGRES_DB}
    volumes: # Make docker client accessible so we can terminate containers from dagster-webserver
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - ./etc/dagster:/opt/dagster/dagster_home/
    depends_on:
      dagster-postgresql:
        condition: service_healthy
      dagster-pipeline:
        condition: service_started

  # This service runs the dagster-daemon process, which is responsible for taking runs
  # off of the queue and launching them, as well as creating runs from schedules or sensors.
  dagster-daemon:
    networks: [ipl]
    image: ${DAGSTER_DAEMON_IMAGE}
    restart: on-failure
    environment:
      PGHOST: dagster-postgresql
      PGUSER: ${DAGSTER_POSTGRES_USER}
      PGPASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      PGDATABASE: ${DAGSTER_POSTGRES_DB}
    volumes: # Make docker client accessible so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - ./etc/dagster:/opt/dagster/dagster_home/
    depends_on:
      dagster-postgresql:
        condition: service_healthy
      dagster-pipeline:
        condition: service_started

  # This service stores GTFS data imported by gtfs-importer.
  gtfs-db:
    networks: [ipl]
    image: ${IPL_GTFS_DB_IMAGE}
    volumes:
      - ./var/gtfs/gtfs-db:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: ${IPL_GTFS_DB_POSTGRES_USER}
      POSTGRES_PASSWORD: ${IPL_GTFS_DB_POSTGRES_PASSWORD}
      POSTGRES_DB: ${IPL_GTFS_DB_POSTGRES_DB}
    restart: on-failure
    healthcheck:
      test: "PGPASSWORD=${IPL_GTFS_DB_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${IPL_GTFS_DB_POSTGRES_USER} -d ${IPL_GTFS_DB_POSTGRES_DB}"
      timeout: 3s
      interval: 5s
      start_period: 10s
      retries: 10

  gtfs-api:
    networks: [ipl]
    depends_on:
      gtfs-db:
        condition: service_healthy
    links:
      - gtfs-db
    image: postgrest/postgrest
    ports:
      - ${IPL_GTFS_API_PORT}:3000
    read_only: true
    # contains the latest import's DB name as `PGDATABASE`
    env_file: ./.imported-gtfs-db.env
    environment:
      PGHOST: gtfs-db
      PGUSER: ${IPL_GTFS_DB_POSTGRES_USER}
      PGPASSWORD: ${IPL_GTFS_DB_POSTGRES_PASSWORD}
      # PostgREST-specific env vars
      PGRST_OPENAPI_SERVER_PROXY_URI: ${IPL_GTFS_API_PUBLIC_BASE_URL}
      PGRST_DB_SCHEMAS: api
      PGRST_DB_ANON_ROLE: web_anon
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.gtfs-api.rule=PathPrefix(`/gtfs`)"
      - "traefik.http.routers.gtfs-api.middlewares=gtfs-stripprefix"
      - "traefik.http.middlewares.gtfs-stripprefix.stripprefix.prefixes=/gtfs"
      - "traefik.http.services.gtfs-api.loadbalancer.server.port=3000"
    restart: unless-stopped

  gtfs-importer:
    networks: [ipl]
    # gtfstidy currently only provides a linux/amd64 binary
    # see also https://github.com/patrickbr/gtfstidy/pull/21
    platform: linux/amd64
    profiles:
      - import-new-gtfs
    depends_on:
      gtfs-db:
        condition: service_healthy
    links:
      - gtfs-db
    build:
      context: ./gtfs-importer
    volumes:
      - ./var/gtfs:/var/gtfs
      - ./etc/gtfs:/etc/gtfs
      - ./.imported-gtfs-db.env:/var/.imported-gtfs-db.env
    environment:
      PGHOST: gtfs-db
      PGUSER: ${IPL_GTFS_DB_POSTGRES_USER}
      PGPASSWORD: ${IPL_GTFS_DB_POSTGRES_PASSWORD}
      # the default DB, used by gtfs-importer to keep track of imports
      PGDATABASE: ${IPL_GTFS_DB_POSTGRES_DB}
      GTFS_DOWNLOAD_URL: ${GTFS_DOWNLOAD_URL:-}
      # the prefix of all DBs created for imports by gtfs-importer
      GTFS_IMPORTER_DB_PREFIX: ${IPL_GTFS_DB_POSTGRES_DB_PREFIX}
      # path to the env file containing the latest import's DB name as `PGDATABASE`
      GTFS_IMPORTER_ENV_FILE: /var/.imported-gtfs-db.env
      GTFS_TMP_DIR: /var/gtfs

  gtfs-swagger-ui:
    networks: [ipl]
    depends_on:
      gtfs-api:
        condition: service_started
    image: swaggerapi/swagger-ui
    ports:
      - ${IPL_GTFS_SWAGGER_UI_PORT}:8080
    environment:
      # Swagger UI will call gtfs-api in the browser, so we need to use its "outside" port here.
      API_URL: http://localhost:${IPL_GTFS_API_PORT}
      BASE_URL: /docs/gtfs
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.gtfs-swagger-ui.rule=PathPrefix(`/docs/gtfs`)"
      - "traefik.http.services.gtfs-swagger-ui.loadbalancer.server.port=8080"
    restart: unless-stopped

networks:
  ipl:
    driver: bridge
    ipam:
      driver: default
      config:
      - subnet: 172.28.0.0/16
        ip_range: 172.28.5.0/24
        gateway: 172.28.5.254
