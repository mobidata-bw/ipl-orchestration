services:
  # This service applies a couple of conversions for certain GBFS feeds to fix
  # syntax issues which prevent importing in Lamassu.
  # It needs to be configured as proxy for Lamassu and feeds to be filtered should
  # explicitly be requests using http. Proxy will take care of https requesting and
  # necessary response rewrites.
  transformer-proxy:
    image: ${TRANSFORMER_PROXY_IMAGE}
    ports:
      - ${TRANSFORMER_PROXY_PORT}:8080
  
  # This service requests non-standard sharing provider APIs and generates
  # a gbfs feed intended for local import into Lamassu. The volume needs to be
  # mounted by the lamassu container also.
  x2gbfs:
    image: ${X2GBFS_IMAGE}
    volumes:
      - ./var/gbfs/feeds:/app/out/
    command: -p deer -b file:///var/gbfs/feeds -i 45
    environment:
      - DEER_API_URL
      - DEER_USER
      - DEER_PASSWORD
  
  lamassu:
    image: ${LAMASSU_IMAGE}
    ports:
      - ${LAMASSU_PORT}:8080
      - ${LAMASSU_ADMIN_PORT}:9001
    environment:
      # depending on base image, JAVA_TOOL_OPTIONS must be used
      JDK_JAVA_OPTIONS: 
        -Dspring.config.location=/etc/application-config/application.properties 
        -Dspring.profiles.active=leader
        -Dorg.entur.lamassu.adminPassword=${LAMASSU_ADMIN_PASSWORD}
        -Dhttp.proxyHost=transformer-proxy
        -Dhttp.proxyPort=8080
    volumes:
      - ./etc/lamassu/:/etc/application-config/
      - ./var/gbfs/feeds:/var/gbfs/feeds
    depends_on:
        redis:
          condition: service_healthy

  redis:
    image: redis:6-alpine
    # Redis tries to use all memory it gets, and doesn't query the container's memory limit.
    # see also https://stackoverflow.com/a/70779280
    command: --maxmemory ${REDIS_MEMORY_MB:-256}mb
    deploy:
      resources:
        limits:
          memory: ${REDIS_MEMORY_MB:-256}M
    # The redis image doesn't provide a health check by itself.
    # see also https://github.com/docker-library/redis/issues/91
    healthcheck:
      test: "redis-cli --raw incr ping"

  ipl-db:
      image: ${IPL_POSTGIS_IMAGE}
      volumes:
        - ./var/postgis:/var/lib/postgresql
      ports:
        - ${IPL_POSTGRES_PORT}:5432
      environment:
        - POSTGRES_DB=${IPL_POSTGRES_DB}
        - POSTGRES_USER=${IPL_POSTGRES_USER}
        - POSTGRES_PASSWORD=${IPL_POSTGRES_PASSWORD}
      restart: on-failure
      healthcheck:
        test: "PGPASSWORD=${IPL_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${IPL_POSTGRES_USER} -d ${IPL_POSTGRES_DB}"

  # This service runs the postgres DB used by dagster for run storage, schedule storage,
  # and event log storage.
  dagster-postgresql:
    image: postgres:${DAGSTER_POSTGRES_IMAGE_TAG}
    environment:
      POSTGRES_USER: "${DAGSTER_POSTGRES_USER}"
      POSTGRES_PASSWORD: "${DAGSTER_POSTGRES_PASSWORD}"
      POSTGRES_DB: "${DAGSTER_POSTGRES_DB}"
    restart: on-failure
    # TODO we should mount volume to allow backup
    healthcheck:
        test: "PGPASSWORD=${DAGSTER_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${DAGSTER_POSTGRES_USER} -d ${DAGSTER_POSTGRES_DB}"

  # This service runs the gRPC server that loads your user code, in both dagster-webserver
  # and dagster-daemon. Multiple containers like this can be deployed separately - each just needs to run on
  # its own port, and have its own entry in the workspace.yaml file that's loaded by 
  # dagster-webserver.
  dagster-pipeline:
    image: registry.git.sectio-aurea.org/mobidata-bw/pipeline/pipeline-code:${DAGSTER_PIPELINE_IMAGE_TAG}
    restart: always
    environment:
      - DAGSTER_POSTGRES_USER
      - DAGSTER_POSTGRES_PASSWORD
      - DAGSTER_POSTGRES_DB
      - IPL_LAMASSU_BASE_URL
      - IPL_POSTGRES_HOST=ipl-db
      - IPL_POSTGRES_PORT
      - IPL_POSTGRES_DB
      - IPL_POSTGRES_USER
      - IPL_POSTGRES_PASSWORD
    depends_on:
        dagster-postgresql:
          condition: service_healthy

  # This service runs dagster-webserver, which loads your user code from the user code container.
  # Since our instance uses the QueuedRunCoordinator, any runs submitted from dagster-webserver
  # will be put on a queue and later dequeued and launched by dagster-daemon.
  dagster-dagit:
    image: registry.git.sectio-aurea.org/mobidata-bw/pipeline/pipeline-dagster:${DAGSTER_IMAGE_TAG}
    entrypoint:
      - dagster-webserver
      - -h
      - "0.0.0.0"
      - -p
      - "3000"
      - -w
      - workspace.yaml
    expose:
      - "3000"
    ports:
      - "3000:3000"
    environment:
      - DAGSTER_POSTGRES_USER
      - DAGSTER_POSTGRES_PASSWORD
      - DAGSTER_POSTGRES_DB
    volumes: # Make docker client accessible so we can terminate containers from dagster-webserver
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - ./etc/dagster:/opt/dagster/dagster_home/
    depends_on:
      dagster-postgresql:
        condition: service_healthy
      dagster-pipeline:
        condition: service_started

  # This service runs the dagster-daemon process, which is responsible for taking runs
  # off of the queue and launching them, as well as creating runs from schedules or sensors.
  dagster-daemon:
    image: registry.git.sectio-aurea.org/mobidata-bw/pipeline/pipeline-dagster:${DAGSTER_IMAGE_TAG}
    entrypoint:
      - dagster-daemon
      - run
    restart: on-failure
    environment:
      - DAGSTER_POSTGRES_USER
      - DAGSTER_POSTGRES_PASSWORD
      - DAGSTER_POSTGRES_DB
    volumes: # Make docker client accessible so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - ./etc/dagster:/opt/dagster/dagster_home/
    depends_on:
      dagster-postgresql:
        condition: service_healthy
      dagster-pipeline:
        condition: service_started

