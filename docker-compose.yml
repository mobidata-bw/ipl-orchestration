# compose "namespace", prepended to all network names, container names, etc.
name: ipl

services:
  # Traefik is our ingress: requests enter our infrastructure through it.
  # It is an HTTP reverse proxy, discovering available services (and their containers)
  # automatically by using the Docker API.
  ingress:
    networks: [ipl]
    image: traefik:v2.10
    ports:
      # todo: change to 80:80
      - "8080:80"
    volumes:
      - ./etc/traefik:/etc/traefik
      - ./var/log/traefik:/var/log/traefik
      # So that Traefik can listen to the Docker events.
      - /var/run/docker.sock:/var/run/docker.sock
    healthcheck:
      test: "traefik healthcheck"

  # This service applies a couple of conversions for certain GBFS feeds to fix
  # syntax issues which prevent importing in Lamassu.
  # It needs to be configured as proxy for Lamassu and feeds to be filtered should
  # explicitly be requests using http. Proxy will take care of https requesting and
  # necessary response rewrites.
  transformer-proxy:
    networks: [ipl]
    image: ${TRANSFORMER_PROXY_IMAGE}
    volumes:
      # Mount config file which declares hosts to proxy
      - ./etc/transformer-proxy/config.yaml:/app/config.yaml
    ports:
      # We export the port for debugging purposes. Lamassu acceses via docker network
      - ${TRANSFORMER_PROXY_PORT}:8080
  
  # This service requests non-standard sharing provider APIs and generates
  # a gbfs feed intended for local import into Lamassu. The volume needs to be
  # mounted by the lamassu container also.
  x2gbfs:
    networks: [ipl]
    image: ${X2GBFS_IMAGE}
    volumes:
      - ./var/gbfs/feeds:/app/out/
    command: -p deer -b file:///var/gbfs/feeds -i 45
    environment:
      - DEER_API_URL
      - DEER_USER
      - DEER_PASSWORD
  
  lamassu:
    networks: [ipl]
    image: ${LAMASSU_IMAGE}
    ports:
      - ${LAMASSU_PORT}:8080
      - ${LAMASSU_ADMIN_PORT}:9001
    environment:
      # depending on base image, JAVA_TOOL_OPTIONS must be used
      LAMASSU_BASE_URL: ${LAMASSU_BASE_URL}
      JDK_JAVA_OPTIONS: 
        -Dspring.config.location=/etc/application-config/application.properties 
        -Dspring.profiles.active=leader
        -Dorg.entur.lamassu.adminPassword=${LAMASSU_ADMIN_PASSWORD}
        -Dhttp.proxyHost=transformer-proxy
        -Dhttp.proxyPort=8080
    volumes:
      - ./etc/lamassu/:/etc/application-config/
      - ./var/gbfs/feeds:/var/gbfs/feeds
    depends_on:
        redis:
          condition: service_healthy
        transformer-proxy:
          condition: service_started

  redis:
    networks: [ipl]
    image: redis:6-alpine
    # Redis tries to use all memory it gets, and doesn't query the container's memory limit.
    # see also https://stackoverflow.com/a/70779280
    command: --maxmemory ${REDIS_MEMORY_MB:-256}mb
    deploy:
      resources:
        limits:
          memory: ${REDIS_MEMORY_MB:-256}M
    # The redis image doesn't provide a health check by itself.
    # see also https://github.com/docker-library/redis/issues/91
    healthcheck:
      test: "redis-cli --raw incr ping"

  ipl-db:
      networks: [ipl]
      image: ${IPL_POSTGIS_IMAGE}
      volumes:
        - ./var/ipl-db/data:/var/lib/postgresql/data
      ports:
        - ${IPL_POSTGRES_PORT}:5432
      environment:
        - POSTGRES_DB=${IPL_POSTGRES_DB}
        - POSTGRES_USER=${IPL_POSTGRES_USER}
        - POSTGRES_PASSWORD=${IPL_POSTGRES_PASSWORD}
      restart: on-failure
      healthcheck:
        test: "PGPASSWORD=${IPL_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${IPL_POSTGRES_USER} -d ${IPL_POSTGRES_DB}"

  # This service runs the postgres DB used by dagster for run storage, schedule storage,
  # and event log storage.
  dagster-postgresql:
    networks: [ipl]
    image: ${DAGSTER_POSTGRES_IMAGE}
    environment:
      POSTGRES_USER: "${DAGSTER_POSTGRES_USER}"
      POSTGRES_PASSWORD: "${DAGSTER_POSTGRES_PASSWORD}"
      POSTGRES_DB: "${DAGSTER_POSTGRES_DB}"
    restart: on-failure
    volumes:
        - ./var/dagster-postgresql/data:/var/lib/postgresql/data
    healthcheck:
        test: "PGPASSWORD=${DAGSTER_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${DAGSTER_POSTGRES_USER} -d ${DAGSTER_POSTGRES_DB}"
        interval: 5s
        timeout: 3s
        retries: 12

  # This service runs the gRPC server that loads your user code, in both dagster-webserver
  # and dagster-daemon. Multiple containers like this can be deployed separately - each just needs to run on
  # its own port, and have its own entry in the workspace.yaml file that's loaded by 
  # dagster-webserver.
  dagster-pipeline:
    networks: [ipl]
    image: ${DAGSTER_PIPELINE_IMAGE}
    restart: always
    environment:
      - PGHOST=dagster-postgresql
      - PGUSER=${DAGSTER_POSTGRES_USER}
      - PGPASSWORD=${DAGSTER_POSTGRES_PASSWORD}
      - PGDATABASE=${DAGSTER_POSTGRES_DB}
      - IPL_LAMASSU_BASE_URL=${LAMASSU_BASE_URL}
      - IPL_POSTGRES_HOST=ipl-db
      - IPL_POSTGRES_PORT
      - IPL_POSTGRES_DB
      - IPL_POSTGRES_USER
      - IPL_POSTGRES_PASSWORD
    depends_on:
        dagster-postgresql:
          condition: service_healthy

  # This service runs dagster-webserver, which loads your user code from the user code container.
  # Since our instance uses the QueuedRunCoordinator, any runs submitted from dagster-webserver
  # will be put on a queue and later dequeued and launched by dagster-daemon.
  dagster-dagit:
    networks: [ipl]
    image: ${DAGSTER_DAGIT_IMAGE}
    expose:
      - "3000"
    ports:
      - "3000:3000"
    environment:
      PGHOST: dagster-postgresql
      PGUSER: ${DAGSTER_POSTGRES_USER}
      PGPASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      PGDATABASE: ${DAGSTER_POSTGRES_DB}
    volumes: # Make docker client accessible so we can terminate containers from dagster-webserver
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - ./etc/dagster:/opt/dagster/dagster_home/
    depends_on:
      dagster-postgresql:
        condition: service_healthy
      dagster-pipeline:
        condition: service_started

  # This service runs the dagster-daemon process, which is responsible for taking runs
  # off of the queue and launching them, as well as creating runs from schedules or sensors.
  dagster-daemon:
    networks: [ipl]
    image: ${DAGSTER_DAEMON_IMAGE}
    restart: on-failure
    environment:
      PGHOST: dagster-postgresql
      PGUSER: ${DAGSTER_POSTGRES_USER}
      PGPASSWORD: ${DAGSTER_POSTGRES_PASSWORD}
      PGDATABASE: ${DAGSTER_POSTGRES_DB}
    volumes: # Make docker client accessible so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
      - ./etc/dagster:/opt/dagster/dagster_home/
    depends_on:
      dagster-postgresql:
        condition: service_healthy
      dagster-pipeline:
        condition: service_started

  # This service stores GTFS data imported by gtfs-importer.
  gtfs-db:
    networks: [ipl]
    image: ${IPL_GTFS_DB_IMAGE}
    volumes:
      - ./var/gtfs/gtfs-db:/var/lib/postgresql
    environment:
      POSTGRES_USER: ${IPL_GTFS_DB_POSTGRES_USER}
      POSTGRES_PASSWORD: ${IPL_GTFS_DB_POSTGRES_PASSWORD}
      POSTGRES_DB: ${IPL_GTFS_DB_POSTGRES_DB}
    restart: on-failure
    healthcheck:
      test: "PGPASSWORD=${IPL_GTFS_DB_POSTGRES_PASSWORD} pg_isready -h 127.0.0.1 -U ${IPL_GTFS_DB_POSTGRES_USER} -d ${IPL_GTFS_DB_POSTGRES_DB}"
      timeout: 3s
      interval: 5s
      start_period: 10s
      retries: 10

  gtfs-api:
    networks: [ipl]
    depends_on:
      gtfs-db:
        condition: service_healthy
    links:
      - gtfs-db
    image: postgrest/postgrest
    ports:
      - ${IPL_GTFS_API_PORT}:3000
    read_only: true
    env_file: ./.gtfs-api.env
    environment:
      PGHOST: gtfs-db
      PGUSER: ${IPL_GTFS_DB_POSTGRES_USER}
      PGPASSWORD: ${IPL_GTFS_DB_POSTGRES_PASSWORD}
      # PostgREST-specific env vars
      PGRST_OPENAPI_SERVER_PROXY_URI: ${IPL_GTFS_API_PUBLIC_BASE_URL}
      PGRST_DB_SCHEMAS: api
      PGRST_DB_ANON_ROLE: web_anon
    restart: unless-stopped

  gtfs-importer:
    networks: [ipl]
    profiles:
      - import-new-gtfs
    depends_on:
      gtfs-db:
        condition: service_healthy
    links:
      - gtfs-db
    build:
      context: ./gtfs-importer
    volumes:
      - ./var/gtfs:/var/gtfs
      - ./.gtfs-api.env:/var/.gtfs-api.env
    environment:
      PGHOST: gtfs-db
      PGUSER: ${IPL_GTFS_DB_POSTGRES_USER}
      PGPASSWORD: ${IPL_GTFS_DB_POSTGRES_PASSWORD}
      # the default DB, used by gtfs-importer to keep track of imports
      PGDATABASE: ${IPL_GTFS_DB_POSTGRES_DB}
      GTFS_DOWNLOAD_URL: ${GTFS_DOWNLOAD_URL:-}
      # the prefix of all DBs created for imports by gtfs-importer
      GTFS_IMPORTER_DB_PREFIX: ${IPL_GTFS_DB_POSTGRES_DB_PREFIX}
      # let the importer modify gtfs-api's env file
      GTFS_IMPORTER_ENV_FILE: /var/.gtfs-api.env
      GTFS_TMP_DIR: /var/gtfs

  gtfs-swagger-ui:
    networks: [ipl]
    depends_on:
      gtfs-api:
        condition: service_started
    image: swaggerapi/swagger-ui
    ports:
      - ${IPL_GTFS_SWAGGER_UI_PORT}:8080
    environment:
      # Swagger UI will call gtfs-api in the browser, so we need to use its "outside" port here.
      API_URL: http://localhost:${IPL_GTFS_API_PORT}
    restart: unless-stopped

networks:
  ipl:
    driver: bridge
    ipam:
      driver: default
      config:
      - subnet: 172.28.0.0/16
        ip_range: 172.28.5.0/24
        gateway: 172.28.5.254
